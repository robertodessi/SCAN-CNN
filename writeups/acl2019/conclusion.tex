\section{Conclusion}

We showed that, compared to the RNNs previously tested in the
literature, the out-of-the-box fairseq CNN architecture reaches
dramatically better performance on the SCAN compositional
generalization tasks. Still, a follow-up analysis suggests that this
network is not really learning rule-like compositional
generalizations, as its mistakes are evenly spread across different
commands and non-systematic.

In future work, we would like to further our insights on the CNN
aspects that are crucial for the task, furthening our preliminary
analyses of kernel widht and attention. We would also like to compare
RNNs and CNNs to self-attentive networks on SCAN.

\mb{Discuss, among other things, number of parameters in RNNs vs
  CNNs.}

