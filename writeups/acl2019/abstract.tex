\begin{abstract}
  The ability to systematically compose new utterances from smaller and known units is a
  key abilty for humans, who are able to master such property withouth being instructed to do so.
  %Conversely, modern neural networks artchitectures are unable to perform even basic
  %compositional operation on unseen words.
  Recurrent neural network models are good at generalizing when they are trained on examples 
  that are similar in structure and length to the ones seen at test time, but their performance
  decrease if length sequence is different between train and test data, finally showing poor performance
  when they have to generalize to an entirely new composed sequence.
  In this paper we report a set of experimental results of several 
  configurations of convolutional neural networks (CNNs)
  on the SCAN dataset \cite{Lake:Baroni:2017}. We find that CNNs largely outperform recurrent neural networks
  and are able to achieve successful performance even on the hardest split of SCAN.
  %Systematic compositionality as defined by Lake and Baroni, 2017,
  %is the "the algebraic capacity to understand and produce a
  %potentially infinite number of novel combinations from known components" 
\end{abstract}
