\begin{abstract}
  \citet{Lake:Baroni:2107} introduced the SCAN dataset to probe the
  ability of seq2seq models to capture compositional generalizations,
  such as those that allow humans to infer the meaning of \emph{``jump
    around''} from the component words, even if they have never heard
  this exact phrase. Lake and Baroni and following researchers focused
  their evaluation on recurrent networks, finding that they are
  utterly failing the most challenging cases of systematic
  compositional generalization. We tested an out-of-the-box
  convolutional networ on such tasks, and we report a 

  The ability to systematically compose new utterances from smaller and known units is a
  key abilty for humans, who are able to master such property withouth being instructed to do so.
  %Conversely, modern neural networks artchitectures are unable to perform even basic
  %compositional operation on unseen words.
  Recurrent neural network models are good at generalizing when they are trained on examples 
  that are similar in structure and length to the ones seen at test time, but their performance
  decrease if length sequence is different between train and test data, finally showing poor performance
  when they have to generalize to an entirely new composed sequence.
  In this paper we report a set of experimental results of several 
  configurations of convolutional neural networks (CNNs)
  on the SCAN dataset \cite{Lake:Baroni:2017}. We find that CNNs largely outperform recurrent neural networks
  and are able to achieve successful performance even on the hardest split of SCAN.
  %Systematic compositionality as defined by Lake and Baroni, 2017,
  %is the "the algebraic capacity to understand and produce a
  %potentially infinite number of novel combinations from known components" 
\end{abstract}
