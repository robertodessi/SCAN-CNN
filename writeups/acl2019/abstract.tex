\begin{abstract}
  \citet{Lake:Baroni:2017} introduced the SCAN dataset probing the
  ability of seq2seq models to capture compositional generalizations,
  such as inferring the meaning of \emph{``jump
    around''} from the component words, even if they are never exposed to
  this exact phrase. Recurrent networks (RNNs) were found to
  completely fail the most challenging generalization cases. We test here
  an out-of-the-box convolutional network (CNN) on these tasks, and we
  report hugely improved performance with respect to RNNs. Error analysis shows that, despite this big improvement, the CNN
  has not induced systematic rules, suggesting that the
  difference between compositional and non-compositional behaviour is
  not clear-cut.
  % The ability to systematically compose new utterances from smaller and known units is a
  % key abilty for humans, who are able to master such property withouth being instructed to do so.
  % %Conversely, modern neural networks artchitectures are unable to perform even basic
  % %compositional operation on unseen words.
  % Recurrent neural network models are good at generalizing when they are trained on examples 
  % that are similar in structure and length to the ones seen at test time, but their performance
  % decrease if length sequence is different between train and test data, finally showing poor performance
  % when they have to generalize to an entirely new composed sequence.
  % In this paper we report a set of experimental results of several 
  % configurations of convolutional neural networks (CNNs)
  % on the SCAN dataset \cite{Lake:Baroni:2017}. We find that CNNs largely outperform recurrent neural networks
  % and are able to achieve successful performance even on the hardest split of SCAN.
  % %Systematic compositionality as defined by Lake and Baroni, 2017,
  %is the "the algebraic capacity to understand and produce a
  %potentially infinite number of novel combinations from known components" 
\end{abstract}
