\section{Introduction}
\label{sec:intro}

The advent of powerful deep neural network has rekindled classic
debates on the natural language processing abilities of such models
\cite[e.g.,][]{Kirov:Cotterell:2018,Linzen:etal:2018,McCoy:etal:2018,Pater:2018}. \citet{Lake:Baroni:2017}
and \citet{Loula:etal:2018} have proposed the SCAN challenge to
directly assess the ability of sequence-to-sequence networks to
perform systematic, compositional generalization of linguistic rules.

%\cite{Bastings:etal:2018}.


Convolutional models have recently proved to be efficient and powerful solutions in machine translation 
\cite{kalchbrenner:etal:2016, gehring:etal:2016, gehring:etal:2017}.
The lack of recurrent connections makes their training faster and more parallelizable 
Self-attention networks have also gained attention recently as they were able to outperform convolutional models on different
machine translation benchmarks \cite{vaswani:etal:2017, chen:etal:2018} while showing a comparable training cost.
Our work is focused on assessing whether a powerful and efficient neural network based on convolutional connections
is capable of learning to compose and generalize to unseen meanings when carefully trained on smaller semantic units.
We decided to make the focus of our work the qualitative and quantitative study of a convolutional architecture, as
it is be easier to inspect its inner components compared to self attention based networks.
We leave for future work similar comparative study that employs self-attentive models.



