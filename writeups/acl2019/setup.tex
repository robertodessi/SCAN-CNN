\section{SCAN dataset and convolutional model}
\label{sec:setup}

The SCAN dataset which was introduced in \cite{Lake:Baroni:2017} tests generalization capabilities of a learning model.
It is designed as a translation task where a sequence of spatial navigation commands have to be translated into corresponding 
spatial navigation actions. The commands are natural language sentences generated by a Generative grammar \rd{\dots}

Convolutional models have recently proved to be efficient and powerful solutions in machine translation 
\cite{kalchbrenner:etal:2016, gehring:etal:2016, gehring:etal:2017}.
The lack of recurrent connections makes their training faster and more parallelizable 
Self-attention networks have also gained attention recently as they were able to outperform convolutional models on different
machine translation benchmarks \cite{vaswani:etal:2017, chen:etal:2018} while showing a comparable training cost.
Our work is focused on assessing whether a powerful and efficient neural network based on convolutional connections
is capable of learning to compose and generalize to unseen meanings when carefully trained on smaller semantic units.
We decided to make the focus of our work the qualitative and quantitative study of a convolutional architecture, as
it is be easier to inspect its inner components compared to self attention based networks.
We leave for future work similar comparative study that employs self-attentive models.

We consider the use of convolutional filters as an interesting dimension to inspect, especially when considering 
its ability to possibly assess the small and self contained semantics of single words within the larger context of a sentence \rd{\dots make a proper example with SCAN]}.
The desidered systematic compositionality that we seek to find in modern neural networks could indeed be described as the
human ability to extract, process and combine small unit that are semantically meaningful, in order to create a larger and again meaningful semantic block.
The hypothesis of the use of convolutions as an important parameter for the ability of the network
to correctly process semantic compositionality is investigated in our second experiment presented in section \ref{subsec:exp2}.

The model used in our experiments is the fully convolutional sequence to sequence neural network introduced in \cite{gehring:etal:2017}.
This model uses convolutional filters and Gated Linear Units \cite{dauphin:etal:2016} along with a multi-step attention mechanism that connects the encoder and the decoder.
The attention module computes different steps (or computations) before calculating its final attention scores at any given timestep, similarily 
to the multi-hop method proposed in \cite{sukhbaatar:etal:2015}.
In order to make our findings comparable with previous results \cite{Lake:Baroni:2017,Loula:etal:2018}, 
we trained our models for one epoch using Nesterovâ€™s accelerated gradient algorithm \cite{sutskever:etal:2013} with a momentum of 0.99 \cite{pascanu:etal:2012} 
and clippied the gradient when it exceeded a norm of 0.1. 




